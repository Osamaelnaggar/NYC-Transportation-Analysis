{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d29410b",
   "metadata": {},
   "source": [
    "âœ… Function Purpose:\n",
    "Input: 'Cleaned_FHVHV.parquet'\n",
    "\n",
    "Target Variables: base_passenger_fare, trip_time\n",
    "\n",
    "Output: X_train, X_test, y_train, y_test\n",
    "\n",
    "âœ… Function Workflow:\n",
    "Load the dataset from Parquet.\n",
    "\n",
    "Feature engineering:\n",
    "\n",
    "Add pickup_unix, pickup_day, is_weekend, pickup_bucket, fare_to_pay_ratio, time_to_pickup, zone_pair.\n",
    "\n",
    "Drop irrelevant columns like timestamps and Borough/ServiceZone.\n",
    "\n",
    "One-hot encode categorical features: fhvhv_type, trip_category, trip_distance_class, pickup_bucket, zone_pair.\n",
    "\n",
    "Handle missing & infinite values.\n",
    "\n",
    "Clip outliers in fare_per_mile and average_speed.\n",
    "\n",
    "Scale numeric features using StandardScaler.\n",
    "\n",
    "Split into train/test sets (80/20)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2756bbc",
   "metadata": {},
   "source": [
    "âœ… Ram Strategy\n",
    "ğŸ”¹ 1. Add Row Sampling (limit the dataset size to fit RAM)\n",
    "Add a row_limit parameter to read only part of the data.\n",
    "\n",
    "ğŸ”¹ 2. Use Sparse Matrices (to avoid huge dense arrays from one-hot encoding)\n",
    "Set sparse_output=True in OneHotEncoder and avoid converting it to DataFrame until needed.\n",
    "\n",
    "ğŸ”¹ 3. Avoid In-Memory Full Export\n",
    "Avoid exporting large X_processed as a dense DataFrame. Save X_train/X_test only, optionally using sparse format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe52390",
   "metadata": {},
   "source": [
    "| File Name                   | Description                          |\n",
    "| --------------------------- | ------------------------------------ |\n",
    "| `FHVHV_X_processed.parquet` | Full features after encoding/scaling |\n",
    "| `FHVHV_y.parquet`           | Full target column                   |\n",
    "| `FHVHV_X_train.parquet`     | Training features                    |\n",
    "| `FHVHV_X_test.parquet`      | Test features                        |\n",
    "| `FHVHV_y_train.parquet`     | Training target                      |\n",
    "| `FHVHV_y_test.parquet`      | Test target                          |\n",
    "| `FHVHV_preprocessor.pkl`    | Saved preprocessor object            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb18f91e",
   "metadata": {},
   "source": [
    " # âœ… Part 1: Preprocessing and exporting cleaned feature-ready dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "083b1949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed and exported 1,000,000 rows.\n",
      "âœ… Full processed features exported (before split).\n",
      "âœ… Train/test sets exported.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "\n",
    "def preprocess_split_export_fhvhv(\n",
    "    parquet_path=\"Cleaned_FHVHV.parquet\",\n",
    "    target_col='base_passenger_fare',\n",
    "    output_prefix='FHVHV',\n",
    "    row_limit=500_000  # âœ… Limit rows for RAM safety\n",
    "):\n",
    "    # Step 1: Load data (with sampling)\n",
    "    df = pd.read_parquet(parquet_path, engine='pyarrow')\n",
    "    if row_limit:\n",
    "        df = df.sample(n=row_limit, random_state=42)\n",
    "\n",
    "    # Step 2: Feature engineering\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], errors='coerce')\n",
    "    df['pickup_hour'] = df['pickup_datetime'].dt.hour\n",
    "    df['pickup_unix'] = df['pickup_datetime'].astype('int64') // 10**9\n",
    "    df['pickup_day'] = df['pickup_datetime'].dt.dayofweek\n",
    "    df['is_weekend'] = df['pickup_day'] >= 5\n",
    "    df['pickup_bucket'] = pd.cut(df['pickup_hour'], bins=[-1, 6, 12, 18, 24],\n",
    "                                 labels=['Night', 'Morning', 'Afternoon', 'Evening'])\n",
    "    df['zone_pair'] = df['PU_Zone'].astype(str) + \"_\" + df['DO_Zone'].astype(str)\n",
    "    df['fare_to_pay_ratio'] = df['base_passenger_fare'] / (df['driver_pay'] + 1e-6)\n",
    "    df['time_to_pickup'] = (\n",
    "        pd.to_datetime(df['pickup_datetime'], errors='coerce') -\n",
    "        pd.to_datetime(df['on_scene_datetime'], errors='coerce')\n",
    "    ).dt.total_seconds()\n",
    "\n",
    "    # Step 3: Drop irrelevant or unused columns\n",
    "    drop_cols = [\n",
    "        'request_datetime', 'on_scene_datetime', 'pickup_datetime', 'dropoff_datetime',\n",
    "        'pickup_hour', 'dropoff_hour',\n",
    "        'PU_Borough', 'PU_ServiceZone', 'DO_Borough', 'DO_ServiceZone'\n",
    "    ]\n",
    "    df.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    # Step 4: Clean and filter\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    df = df[(df['fare_per_mile'] > 0) & (df['fare_per_mile'] < 50)]\n",
    "    df = df[(df['average_speed'] > 0) & (df['average_speed'] < 80)]\n",
    "    df = df[df[target_col].notna()]\n",
    "\n",
    "    # Step 5: Separate features and target\n",
    "    y = df[target_col]\n",
    "    X = df.drop(columns=[target_col])\n",
    "\n",
    "    # Step 6: Set categorical flags\n",
    "    flag_cols = [\n",
    "        'shared_request_flag', 'shared_match_flag',\n",
    "        'access_a_ride_flag', 'wav_request_flag', 'wav_match_flag'\n",
    "    ]\n",
    "    for col in flag_cols:\n",
    "        if col in X.columns:\n",
    "            X[col] = X[col].astype(\"category\")\n",
    "\n",
    "    # Step 7: Identify column types\n",
    "    numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # Step 8: Build preprocessor (sparse)\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=True), categorical_cols)\n",
    "    ])\n",
    "\n",
    "    # Step 9: Fit-transform entire dataset\n",
    "    X_processed = preprocessor.fit_transform(X)  # will be sparse\n",
    "\n",
    "    # âœ… Step 10: Export full processed feature set BEFORE split\n",
    "    sparse.save_npz(f\"{output_prefix}_X_processed.npz\", X_processed)\n",
    "    y.to_frame().to_parquet(f\"{output_prefix}_y.parquet\", index=False)\n",
    "\n",
    "    # Step 11: Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_processed, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Step 12: Export splits\n",
    "    sparse.save_npz(f\"{output_prefix}_X_train.npz\", X_train)\n",
    "    sparse.save_npz(f\"{output_prefix}_X_test.npz\", X_test)\n",
    "    y_train.to_frame().to_parquet(f\"{output_prefix}_y_train.parquet\", index=False)\n",
    "    y_test.to_frame().to_parquet(f\"{output_prefix}_y_test.parquet\", index=False)\n",
    "\n",
    "    # Step 13: Save preprocessor\n",
    "    joblib.dump(preprocessor, f\"{output_prefix}_preprocessor.pkl\")\n",
    "\n",
    "    print(f\"âœ… Processed and exported {row_limit:,} rows.\")\n",
    "    print(\"âœ… Full processed features exported (before split).\")\n",
    "    print(\"âœ… Train/test sets exported.\")\n",
    "    return X_train, X_test, y_train, y_test, preprocessor\n",
    "\n",
    "X_train, X_test, y_train, y_test, preprocessor = preprocess_split_export_fhvhv(\n",
    "    parquet_path='Cleaned_FHVHV.parquet',\n",
    "    target_col='base_passenger_fare',\n",
    "    output_prefix='FHVHV',\n",
    "    row_limit=1000000  # Or lower to avoid memory issues\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
